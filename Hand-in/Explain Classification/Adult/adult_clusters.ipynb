{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import hdbscan\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    5000\n",
      "1.0    5000\n",
      "Name: target, dtype: int64\n",
      "Number of Observations: 10000\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n",
    "\n",
    "df = X.copy()\n",
    "df['y'] = y\n",
    " \n",
    "df=df.dropna()\n",
    "\n",
    "df['target'] = df['y'].map({'<=50K': 0, '<=50K.': 0, '>50K': 1, '>50K.': 1})\n",
    "df = df.drop('y', axis=1)\n",
    "\n",
    "df_num = df[['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week', 'target']]\n",
    "\n",
    "# Function to sample 1000 rows from each group\n",
    "def sample_from_group(group):\n",
    "    return group.sample(min(len(group), 5000), random_state=42)\n",
    "\n",
    "# Apply the function to each group defined by the unique values in the 'target' column\n",
    "sampled_df = df_num.groupby('target', group_keys=False).apply(sample_from_group)\n",
    "\n",
    "# Reset index\n",
    "sampled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the scaler on your numerical DataFrame\n",
    "df_num_scaled = scaler.fit_transform(sampled_df)\n",
    "\n",
    "# Convert the scaled array back to a DataFrame\n",
    "df_num = pd.DataFrame(df_num_scaled, columns=df_num.columns)\n",
    "\n",
    "# Count occurrences of each unique value in the 'target' column\n",
    "target_counts = df_num['target'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(target_counts)\n",
    "\n",
    "num_observations = df_num.shape[0]\n",
    "print(\"Number of Observations:\", num_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_num.drop('target', axis=1)\n",
    "y = df_num['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a new DataFrame with unique values and their counts\n",
    "def create_unique_count_df(column_name):\n",
    "    if column_name not in X.columns:\n",
    "        print(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    unique_values = X[column_name].unique()\n",
    "    value_counts = X[column_name].value_counts()\n",
    "    unique_count_df = pd.DataFrame({'Value': unique_values, 'Count': value_counts})\n",
    "    return unique_count_df\n",
    "\n",
    "# Get the column names\n",
    "columns = X.columns\n",
    "\n",
    "# Dictionary to store unique values and counts DataFrames for each column\n",
    "unique_values_counts_dict = {}\n",
    "\n",
    "# Iterate over each column, create a new DataFrame, and store it in the dictionary\n",
    "for col in columns:\n",
    "    unique_count_df = create_unique_count_df(col)\n",
    "    unique_values_counts_dict[col] = unique_count_df\n",
    "\n",
    "# Save each DataFrame for each column to separate files\n",
    "for col, df_count in unique_values_counts_dict.items():\n",
    "    csv_file_name = f\"{col}_unique_values_counts.csv\"\n",
    "    df_count.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Interval  Count\n",
      "0   (0.0, 0.05]    355\n",
      "1   (0.05, 0.1]    468\n",
      "2   (0.1, 0.15]    190\n",
      "3   (0.15, 0.2]    657\n",
      "4   (0.2, 0.25]    133\n",
      "5   (0.25, 0.3]    742\n",
      "6   (0.3, 0.35]    277\n",
      "7   (0.35, 0.4]   1317\n",
      "8   (0.4, 0.45]    484\n",
      "9   (0.45, 0.5]    239\n",
      "10  (0.5, 0.55]     36\n",
      "11  (0.55, 0.6]   4510\n",
      "12  (0.6, 0.65]    371\n",
      "13  (0.65, 0.7]     36\n",
      "14  (0.7, 0.75]     76\n",
      "15  (0.75, 0.8]     15\n",
      "16  (0.8, 0.85]     23\n",
      "17  (0.85, 0.9]     11\n",
      "18  (0.9, 0.95]      4\n",
      "19  (0.95, 1.0]     45\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Hours-per-week_unique_values_counts.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Define bins for the intervals\n",
    "bins = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "# Create intervals and sum 'Count' values within each interval\n",
    "df['Interval'] = pd.cut(df['Value'], bins=bins, include_lowest=False)\n",
    "result_df = df.groupby('Interval')['Count'].sum().reset_index()\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(\"hours_all.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "          Interval  Count\n",
      "0   (-0.001, 0.05]    432\n",
      "1      (0.05, 0.1]    659\n",
      "2      (0.1, 0.15]    577\n",
      "3      (0.15, 0.2]    955\n",
      "4      (0.2, 0.25]   1121\n",
      "5      (0.25, 0.3]    905\n",
      "6      (0.3, 0.35]   1138\n",
      "7      (0.35, 0.4]   1106\n",
      "8      (0.4, 0.45]    699\n",
      "9      (0.45, 0.5]    828\n",
      "10     (0.5, 0.55]    562\n",
      "11     (0.55, 0.6]    346\n",
      "12     (0.6, 0.65]    316\n",
      "13     (0.65, 0.7]    173\n",
      "14     (0.7, 0.75]     79\n",
      "15     (0.75, 0.8]     54\n",
      "16     (0.8, 0.85]     23\n",
      "17     (0.85, 0.9]     12\n",
      "18     (0.9, 0.95]      1\n",
      "19     (0.95, 1.0]     14\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000020D46B99280>\n",
      "fnlwgt\n",
      "          Interval  Count\n",
      "0   (-0.001, 0.05]   1134\n",
      "1      (0.05, 0.1]   2005\n",
      "2      (0.1, 0.15]   2784\n",
      "3      (0.15, 0.2]   2009\n",
      "4      (0.2, 0.25]    905\n",
      "5      (0.25, 0.3]    622\n",
      "6      (0.3, 0.35]    310\n",
      "7      (0.35, 0.4]    111\n",
      "8      (0.4, 0.45]     50\n",
      "9      (0.45, 0.5]     39\n",
      "10     (0.5, 0.55]     12\n",
      "11     (0.55, 0.6]      6\n",
      "12     (0.6, 0.65]      9\n",
      "13     (0.65, 0.7]      2\n",
      "14     (0.7, 0.75]      0\n",
      "15     (0.75, 0.8]      0\n",
      "16     (0.8, 0.85]      1\n",
      "17     (0.85, 0.9]      0\n",
      "18     (0.9, 0.95]      0\n",
      "19     (0.95, 1.0]      1\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000020D46BE7520>\n",
      "capital-gain\n",
      "          Interval  Count\n",
      "0   (-0.001, 0.05]   9010\n",
      "1      (0.05, 0.1]    474\n",
      "2      (0.1, 0.15]    115\n",
      "3      (0.15, 0.2]    227\n",
      "4      (0.2, 0.25]     22\n",
      "5      (0.25, 0.3]     36\n",
      "6      (0.3, 0.35]      1\n",
      "7      (0.35, 0.4]      0\n",
      "8      (0.4, 0.45]      0\n",
      "9      (0.45, 0.5]      0\n",
      "10     (0.5, 0.55]      0\n",
      "11     (0.55, 0.6]      0\n",
      "12     (0.6, 0.65]      0\n",
      "13     (0.65, 0.7]      0\n",
      "14     (0.7, 0.75]      0\n",
      "15     (0.75, 0.8]      0\n",
      "16     (0.8, 0.85]      0\n",
      "17     (0.85, 0.9]      0\n",
      "18     (0.9, 0.95]      0\n",
      "19     (0.95, 1.0]    115\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000020D46B99FA0>\n",
      "capital-loss\n",
      "          Interval  Count\n",
      "0   (-0.001, 0.05]   9387\n",
      "1      (0.05, 0.1]      0\n",
      "2      (0.1, 0.15]      0\n",
      "3      (0.15, 0.2]      3\n",
      "4      (0.2, 0.25]      1\n",
      "5      (0.25, 0.3]      0\n",
      "6      (0.3, 0.35]      1\n",
      "7      (0.35, 0.4]      9\n",
      "8      (0.4, 0.45]     79\n",
      "9      (0.45, 0.5]     40\n",
      "10     (0.5, 0.55]    370\n",
      "11     (0.55, 0.6]     15\n",
      "12     (0.6, 0.65]     35\n",
      "13     (0.65, 0.7]     50\n",
      "14     (0.7, 0.75]      2\n",
      "15     (0.75, 0.8]      6\n",
      "16     (0.8, 0.85]      1\n",
      "17     (0.85, 0.9]      0\n",
      "18     (0.9, 0.95]      0\n",
      "19     (0.95, 1.0]      1\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000020D46BE7910>\n",
      "hours-per-week\n",
      "          Interval  Count\n",
      "0   (-0.001, 0.05]     54\n",
      "1      (0.05, 0.1]    120\n",
      "2      (0.1, 0.15]    144\n",
      "3      (0.15, 0.2]    305\n",
      "4      (0.2, 0.25]    234\n",
      "5      (0.25, 0.3]    297\n",
      "6      (0.3, 0.35]    426\n",
      "7      (0.35, 0.4]   4731\n",
      "8      (0.4, 0.45]    893\n",
      "9      (0.45, 0.5]   1388\n",
      "10     (0.5, 0.55]     74\n",
      "11     (0.55, 0.6]    359\n",
      "12     (0.6, 0.65]    593\n",
      "13     (0.65, 0.7]    111\n",
      "14     (0.7, 0.75]    128\n",
      "15     (0.75, 0.8]     28\n",
      "16     (0.8, 0.85]     57\n",
      "17     (0.85, 0.9]      8\n",
      "18     (0.9, 0.95]     16\n",
      "19     (0.95, 1.0]     34\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000020D46BE7D00>\n"
     ]
    }
   ],
   "source": [
    "list = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "for l in list:\n",
    "    print(l)\n",
    "    #file_path = 'age_unique_values_counts.csv'  \n",
    "    df = X[l].copy()\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Define bins for the intervals\n",
    "    bins = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "    # Create intervals and sum 'Count' values within each interval\n",
    "    df['Interval'] = pd.cut(df[l], bins=bins, include_lowest=True)\n",
    "\n",
    "    # Group by 'Interval' and count the number of occurrences\n",
    "    bin_counts = df.groupby('Interval').size().reset_index(name='Count')\n",
    "\n",
    "    # Sort bins by interval\n",
    "    bin_counts['Interval'] = bin_counts['Interval'].astype(str)\n",
    "    bin_counts = bin_counts.sort_values(by='Interval')\n",
    "\n",
    "    print(bin_counts)\n",
    "\n",
    "    ordered_df = df.groupby('Interval')\n",
    "    print(ordered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 3\n",
      "Cluster 0:\n",
      "  Total observations: 609\n",
      "  Observations in class 0: 137\n",
      "  Observations in class 1: 472\n",
      "  Purity score: 0.78\n",
      "Cluster 1:\n",
      "  Total observations: 5173\n",
      "  Observations in class 0: 3300\n",
      "  Observations in class 1: 1873\n",
      "  Purity score: 0.64\n",
      "Cluster 2:\n",
      "  Total observations: 4218\n",
      "  Observations in class 0: 1563\n",
      "  Observations in class 1: 2655\n",
      "  Purity score: 0.63\n",
      "Overall purity score: 0.68\n",
      "\n",
      "Number of clusters: 4\n",
      "Cluster 0:\n",
      "  Total observations: 609\n",
      "  Observations in class 0: 137\n",
      "  Observations in class 1: 472\n",
      "  Purity score: 0.78\n",
      "Cluster 1:\n",
      "  Total observations: 5147\n",
      "  Observations in class 0: 3295\n",
      "  Observations in class 1: 1852\n",
      "  Purity score: 0.64\n",
      "Cluster 2:\n",
      "  Total observations: 4129\n",
      "  Observations in class 0: 1568\n",
      "  Observations in class 1: 2561\n",
      "  Purity score: 0.62\n",
      "Cluster 3:\n",
      "  Total observations: 115\n",
      "  Observations in class 0: 0\n",
      "  Observations in class 1: 115\n",
      "  Purity score: 1.00\n",
      "Overall purity score: 0.76\n",
      "\n",
      "Number of clusters: 5\n",
      "Cluster 0:\n",
      "  Total observations: 608\n",
      "  Observations in class 0: 136\n",
      "  Observations in class 1: 472\n",
      "  Purity score: 0.78\n",
      "Cluster 1:\n",
      "  Total observations: 3944\n",
      "  Observations in class 0: 1518\n",
      "  Observations in class 1: 2426\n",
      "  Purity score: 0.62\n",
      "Cluster 2:\n",
      "  Total observations: 2461\n",
      "  Observations in class 0: 1042\n",
      "  Observations in class 1: 1419\n",
      "  Purity score: 0.58\n",
      "Cluster 3:\n",
      "  Total observations: 115\n",
      "  Observations in class 0: 0\n",
      "  Observations in class 1: 115\n",
      "  Purity score: 1.00\n",
      "Cluster 4:\n",
      "  Total observations: 2872\n",
      "  Observations in class 0: 2304\n",
      "  Observations in class 1: 568\n",
      "  Purity score: 0.80\n",
      "Overall purity score: 0.75\n",
      "\n",
      "Number of clusters: 6\n",
      "Cluster 0:\n",
      "  Total observations: 608\n",
      "  Observations in class 0: 136\n",
      "  Observations in class 1: 472\n",
      "  Purity score: 0.78\n",
      "Cluster 1:\n",
      "  Total observations: 3571\n",
      "  Observations in class 0: 1552\n",
      "  Observations in class 1: 2019\n",
      "  Purity score: 0.57\n",
      "Cluster 2:\n",
      "  Total observations: 2150\n",
      "  Observations in class 0: 903\n",
      "  Observations in class 1: 1247\n",
      "  Purity score: 0.58\n",
      "Cluster 3:\n",
      "  Total observations: 115\n",
      "  Observations in class 0: 0\n",
      "  Observations in class 1: 115\n",
      "  Purity score: 1.00\n",
      "Cluster 4:\n",
      "  Total observations: 2516\n",
      "  Observations in class 0: 2065\n",
      "  Observations in class 1: 451\n",
      "  Purity score: 0.82\n",
      "Cluster 5:\n",
      "  Total observations: 1040\n",
      "  Observations in class 0: 344\n",
      "  Observations in class 1: 696\n",
      "  Purity score: 0.67\n",
      "Overall purity score: 0.74\n",
      "\n",
      "Number of clusters: 7\n",
      "Cluster 0:\n",
      "  Total observations: 608\n",
      "  Observations in class 0: 136\n",
      "  Observations in class 1: 472\n",
      "  Purity score: 0.78\n",
      "Cluster 1:\n",
      "  Total observations: 2980\n",
      "  Observations in class 0: 1282\n",
      "  Observations in class 1: 1698\n",
      "  Purity score: 0.57\n",
      "Cluster 2:\n",
      "  Total observations: 2006\n",
      "  Observations in class 0: 836\n",
      "  Observations in class 1: 1170\n",
      "  Purity score: 0.58\n",
      "Cluster 3:\n",
      "  Total observations: 115\n",
      "  Observations in class 0: 0\n",
      "  Observations in class 1: 115\n",
      "  Purity score: 1.00\n",
      "Cluster 4:\n",
      "  Total observations: 1023\n",
      "  Observations in class 0: 521\n",
      "  Observations in class 1: 502\n",
      "  Purity score: 0.51\n",
      "Cluster 5:\n",
      "  Total observations: 991\n",
      "  Observations in class 0: 326\n",
      "  Observations in class 1: 665\n",
      "  Purity score: 0.67\n",
      "Cluster 6:\n",
      "  Total observations: 2277\n",
      "  Observations in class 0: 1899\n",
      "  Observations in class 1: 378\n",
      "  Purity score: 0.83\n",
      "Overall purity score: 0.71\n",
      "\n",
      "Number of clusters: 8\n",
      "Cluster 0:\n",
      "  Total observations: 604\n",
      "  Observations in class 0: 132\n",
      "  Observations in class 1: 472\n",
      "  Purity score: 0.78\n",
      "Cluster 1:\n",
      "  Total observations: 2662\n",
      "  Observations in class 0: 1797\n",
      "  Observations in class 1: 865\n",
      "  Purity score: 0.68\n",
      "Cluster 2:\n",
      "  Total observations: 1329\n",
      "  Observations in class 0: 613\n",
      "  Observations in class 1: 716\n",
      "  Purity score: 0.54\n",
      "Cluster 3:\n",
      "  Total observations: 2756\n",
      "  Observations in class 0: 1086\n",
      "  Observations in class 1: 1670\n",
      "  Purity score: 0.61\n",
      "Cluster 4:\n",
      "  Total observations: 648\n",
      "  Observations in class 0: 614\n",
      "  Observations in class 1: 34\n",
      "  Purity score: 0.95\n",
      "Cluster 5:\n",
      "  Total observations: 924\n",
      "  Observations in class 0: 261\n",
      "  Observations in class 1: 663\n",
      "  Purity score: 0.72\n",
      "Cluster 6:\n",
      "  Total observations: 115\n",
      "  Observations in class 0: 0\n",
      "  Observations in class 1: 115\n",
      "  Purity score: 1.00\n",
      "Cluster 7:\n",
      "  Total observations: 962\n",
      "  Observations in class 0: 497\n",
      "  Observations in class 1: 465\n",
      "  Purity score: 0.52\n",
      "Overall purity score: 0.72\n",
      "\n",
      "Number of clusters: 9\n",
      "Cluster 0:\n",
      "  Total observations: 604\n",
      "  Observations in class 0: 132\n",
      "  Observations in class 1: 472\n",
      "  Purity score: 0.78\n",
      "Cluster 1:\n",
      "  Total observations: 2422\n",
      "  Observations in class 0: 1688\n",
      "  Observations in class 1: 734\n",
      "  Purity score: 0.70\n",
      "Cluster 2:\n",
      "  Total observations: 1174\n",
      "  Observations in class 0: 585\n",
      "  Observations in class 1: 589\n",
      "  Purity score: 0.50\n",
      "Cluster 3:\n",
      "  Total observations: 2590\n",
      "  Observations in class 0: 1082\n",
      "  Observations in class 1: 1508\n",
      "  Purity score: 0.58\n",
      "Cluster 4:\n",
      "  Total observations: 625\n",
      "  Observations in class 0: 596\n",
      "  Observations in class 1: 29\n",
      "  Purity score: 0.95\n",
      "Cluster 5:\n",
      "  Total observations: 854\n",
      "  Observations in class 0: 186\n",
      "  Observations in class 1: 668\n",
      "  Purity score: 0.78\n",
      "Cluster 6:\n",
      "  Total observations: 115\n",
      "  Observations in class 0: 0\n",
      "  Observations in class 1: 115\n",
      "  Purity score: 1.00\n",
      "Cluster 7:\n",
      "  Total observations: 936\n",
      "  Observations in class 0: 477\n",
      "  Observations in class 1: 459\n",
      "  Purity score: 0.51\n",
      "Cluster 8:\n",
      "  Total observations: 680\n",
      "  Observations in class 0: 254\n",
      "  Observations in class 1: 426\n",
      "  Purity score: 0.63\n",
      "Overall purity score: 0.71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "number_clusters = [3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "for n in number_clusters:\n",
    "    kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Create a new DataFrame with the cluster labels\n",
    "    X_n = df_num.copy()\n",
    "    X_n['cluster'] = cluster_labels\n",
    "\n",
    "    print(f\"Number of clusters: {n}\")\n",
    "\n",
    "    for cluster in range(n):\n",
    "        cluster_data = X_n[X_n['cluster'] == cluster]\n",
    "        total_in_cluster = len(cluster_data)\n",
    "        class_0_in_cluster = len(cluster_data[cluster_data['target'] == 0])\n",
    "        class_1_in_cluster = len(cluster_data[cluster_data['target'] == 1])\n",
    "        \n",
    "        # Calculate purity score for the current cluster\n",
    "        if total_in_cluster > 0:\n",
    "            majority_class = cluster_data['target'].mode()[0]\n",
    "            correct_predictions = len(cluster_data[cluster_data['target'] == majority_class])\n",
    "            cluster_purity_score = correct_predictions / total_in_cluster\n",
    "        else:\n",
    "            cluster_purity_score = 0\n",
    "\n",
    "        print(f\"Cluster {cluster}:\")\n",
    "        print(f\"  Total observations: {total_in_cluster}\")\n",
    "        print(f\"  Observations in class 0: {class_0_in_cluster}\")\n",
    "        print(f\"  Observations in class 1: {class_1_in_cluster}\")\n",
    "        print(f\"  Purity score: {cluster_purity_score:.2f}\")\n",
    "\n",
    "    # Calculate overall purity score\n",
    "    purity_scores = []\n",
    "    for cluster in range(n):\n",
    "        cluster_data = X_n[X_n['cluster'] == cluster]\n",
    "        if len(cluster_data) > 0:\n",
    "            majority_class = cluster_data['target'].mode()[0]\n",
    "            correct_predictions = len(cluster_data[cluster_data['target'] == majority_class])\n",
    "            purity_scores.append(correct_predictions / len(cluster_data))\n",
    "    \n",
    "    overall_purity_score = sum(purity_scores) / n\n",
    "    print(f\"Overall purity score: {overall_purity_score:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Purity for 3 clusters: 0.68\n",
      "0, 0.78, 609, 137/472\n",
      "1, 0.64, 5173, 3300/1873\n",
      "2, 0.63, 4218, 1563/2655\n",
      "Overall Purity for 4 clusters: 0.76\n",
      "0, 0.78, 609, 137/472\n",
      "1, 0.64, 5147, 3295/1852\n",
      "2, 0.62, 4129, 1568/2561\n",
      "3, 1.00, 115, 0/115\n",
      "Overall Purity for 5 clusters: 0.75\n",
      "0, 0.78, 608, 136/472\n",
      "1, 0.62, 3944, 1518/2426\n",
      "2, 0.58, 2461, 1042/1419\n",
      "3, 1.00, 115, 0/115\n",
      "4, 0.80, 2872, 2304/568\n",
      "Overall Purity for 6 clusters: 0.74\n",
      "0, 0.78, 608, 136/472\n",
      "1, 0.57, 3571, 1552/2019\n",
      "2, 0.58, 2150, 903/1247\n",
      "3, 1.00, 115, 0/115\n",
      "4, 0.82, 2516, 2065/451\n",
      "5, 0.67, 1040, 344/696\n",
      "Overall Purity for 7 clusters: 0.71\n",
      "0, 0.78, 608, 136/472\n",
      "1, 0.57, 2980, 1282/1698\n",
      "2, 0.58, 2006, 836/1170\n",
      "3, 1.00, 115, 0/115\n",
      "4, 0.51, 1023, 521/502\n",
      "5, 0.67, 991, 326/665\n",
      "6, 0.83, 2277, 1899/378\n",
      "Overall Purity for 8 clusters: 0.72\n",
      "0, 0.78, 604, 132/472\n",
      "1, 0.68, 2662, 1797/865\n",
      "2, 0.54, 1329, 613/716\n",
      "3, 0.61, 2756, 1086/1670\n",
      "4, 0.95, 648, 614/34\n",
      "5, 0.72, 924, 261/663\n",
      "6, 1.00, 115, 0/115\n",
      "7, 0.52, 962, 497/465\n",
      "Overall Purity for 9 clusters: 0.71\n",
      "0, 0.78, 604, 132/472\n",
      "1, 0.70, 2422, 1688/734\n",
      "2, 0.50, 1174, 585/589\n",
      "3, 0.58, 2590, 1082/1508\n",
      "4, 0.95, 625, 596/29\n",
      "5, 0.78, 854, 186/668\n",
      "6, 1.00, 115, 0/115\n",
      "7, 0.51, 936, 477/459\n",
      "8, 0.63, 680, 254/426\n"
     ]
    }
   ],
   "source": [
    "true_labels = y\n",
    "\n",
    "def calculate_purity_and_sizes(cluster_labels, true_labels):\n",
    "    cluster_purities = []\n",
    "    cluster_sizes = []\n",
    "    class_counts = []\n",
    "    for cluster_label in np.unique(cluster_labels):\n",
    "        cluster_indices = np.where(cluster_labels == cluster_label)[0]\n",
    "        cluster_X = X.iloc[cluster_indices]\n",
    "        cluster_labels_true = true_labels.iloc[cluster_indices]\n",
    "        label_counts = np.bincount(cluster_labels_true)\n",
    "        majority_label_count = np.max(label_counts)\n",
    "        cluster_purity = majority_label_count / len(cluster_labels_true)\n",
    "        cluster_purities.append(cluster_purity)\n",
    "        cluster_sizes.append(len(cluster_indices))\n",
    "        class_counts.append(label_counts)\n",
    "    overall_purity = np.mean(cluster_purities)\n",
    "    return overall_purity, cluster_purities, cluster_sizes, class_counts\n",
    "\n",
    "# Perform KMeans clustering for 3, 4, and 5 clusters\n",
    "cluster_numbers = [3, 4, 5, 6, 7, 8, 9]\n",
    "overall_purities = []\n",
    "cluster_purities_iterations = []\n",
    "cluster_sizes_iterations = []\n",
    "class_counts_iterations = []\n",
    "\n",
    "for n_clusters in cluster_numbers:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Calculate purity, sizes, and class counts for each cluster\n",
    "    overall_purity, cluster_purities, cluster_sizes, class_counts = calculate_purity_and_sizes(cluster_labels, true_labels)\n",
    "    overall_purities.append(overall_purity)\n",
    "    cluster_purities_iterations.append(cluster_purities)\n",
    "    cluster_sizes_iterations.append(cluster_sizes)\n",
    "    class_counts_iterations.append(class_counts)\n",
    "\n",
    "for i, n_clusters in enumerate(cluster_numbers):\n",
    "    print(f\"Overall Purity for {n_clusters} clusters: {overall_purities[i]:.2f}\")\n",
    "    for cluster_label, (purity, size, class_count) in enumerate(zip(cluster_purities_iterations[i], cluster_sizes_iterations[i], class_counts_iterations[i])):\n",
    "        class_0_count = class_count[0] if len(class_count) > 0 else 0\n",
    "        class_1_count = class_count[1] if len(class_count) > 1 else 0\n",
    "        print(f\"{cluster_label}, {purity:.2f}, {size}, {class_0_count}/{class_1_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           age    fnlwgt  capital-gain  capital-loss  hours-per-week\n",
      "0     0.246575  0.020062       0.03103      0.000000        0.551020\n",
      "1     0.301370  0.254985       0.00000      0.000000        0.397959\n",
      "2     0.068493  0.353789       0.00000      0.000000        0.397959\n",
      "3     0.465753  0.144167       0.00000      0.000000        0.295918\n",
      "4     0.000000  0.075273       0.01055      0.000000        0.193878\n",
      "...        ...       ...           ...           ...             ...\n",
      "9995  0.191781  0.103344       0.00000      0.000000        0.397959\n",
      "9996  0.465753  0.313124       0.00000      0.000000        0.071429\n",
      "9997  0.150685  0.044391       0.00000      0.424654        0.500000\n",
      "9998  0.356164  0.143599       0.00000      0.000000        0.346939\n",
      "9999  0.575342  0.145647       0.00000      0.000000        0.346939\n",
      "\n",
      "[10000 rows x 5 columns]\n",
      "           age    fnlwgt  capital-gain  capital-loss  hours-per-week  cluster\n",
      "0     0.246575  0.020062       0.03103      0.000000        0.551020        8\n",
      "1     0.301370  0.254985       0.00000      0.000000        0.397959        7\n",
      "2     0.068493  0.353789       0.00000      0.000000        0.397959        7\n",
      "3     0.465753  0.144167       0.00000      0.000000        0.295918        3\n",
      "4     0.000000  0.075273       0.01055      0.000000        0.193878        4\n",
      "...        ...       ...           ...           ...             ...      ...\n",
      "9995  0.191781  0.103344       0.00000      0.000000        0.397959        1\n",
      "9996  0.465753  0.313124       0.00000      0.000000        0.071429        2\n",
      "9997  0.150685  0.044391       0.00000      0.424654        0.500000        0\n",
      "9998  0.356164  0.143599       0.00000      0.000000        0.346939        3\n",
      "9999  0.575342  0.145647       0.00000      0.000000        0.346939        2\n",
      "\n",
      "[10000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=9, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "X_9 = X.copy()\n",
    "print(X)\n",
    "X_9['cluster'] = cluster_labels\n",
    "print(X_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2_df = X_9[X_9['cluster'] == 2][['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']]\n",
    "cluster_7_df = X_9[X_9['cluster'] == 7][['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 9, 67, 309, 240, 243, 146, 65, 48, 22, 11, 1, 13]\n",
      "fnlwgt\n",
      "[121, 232, 351, 239, 118, 56, 32, 13, 7, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "capital-gain\n",
      "[1041, 59, 23, 31, 18, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "capital-loss\n",
      "[1172, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "hours-per-week\n",
      "[26, 43, 42, 76, 63, 57, 75, 686, 77, 28, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "list = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "for l in list:\n",
    "    print(l)\n",
    "    #file_path = 'age_unique_values_counts.csv'  \n",
    "    df = cluster_2_df[l].copy()\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    # Define bins for the intervals\n",
    "    bins = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "    # Create intervals and sum 'Count' values within each interval\n",
    "    df['Interval'] = pd.cut(df[l], bins=bins, include_lowest=True)\n",
    "\n",
    "    # Group by 'Interval' and count the number of occurrences\n",
    "    bin_counts = df.groupby('Interval').size().reset_index(name='Count')\n",
    "\n",
    "    # Sort bins by interval\n",
    "    bin_counts['Interval'] = bin_counts['Interval'].astype(str)\n",
    "    bin_counts = bin_counts.sort_values(by='Interval')\n",
    "\n",
    "    count_list = bin_counts['Count'].tolist()\n",
    "    print(count_list)\n",
    "\n",
    "    ordered_df = df.groupby('Interval')\n",
    "    #print(ordered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a new DataFrame with unique values and their counts\n",
    "def create_unique_count_df(column_name):\n",
    "    if column_name not in cluster_2_df.columns:\n",
    "        print(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    unique_values = cluster_2_df[column_name].unique()\n",
    "    value_counts = cluster_2_df[column_name].value_counts()\n",
    "    unique_count_df = pd.DataFrame({'Value': unique_values, 'Count': value_counts})\n",
    "    return unique_count_df\n",
    "\n",
    "# Get the column names\n",
    "columns = cluster_2_df.columns\n",
    "\n",
    "# Dictionary to store unique values and counts DataFrames for each column\n",
    "unique_values_counts_dict = {}\n",
    "\n",
    "# Iterate over each column, create a new DataFrame, and store it in the dictionary\n",
    "for col in columns:\n",
    "    unique_count_df = create_unique_count_df(col)\n",
    "    unique_values_counts_dict[col] = unique_count_df\n",
    "\n",
    "# Save each DataFrame for each column to separate files\n",
    "for col, df_count in unique_values_counts_dict.items():\n",
    "    csv_file_name = f\"{col}_unique_values_counts.csv\"\n",
    "    df_count.to_csv(csv_file_name, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval\n",
      "(0.0, 0.05]    720\n",
      "(0.05, 0.1]     40\n",
      "(0.1, 0.15]    122\n",
      "(0.15, 0.2]     74\n",
      "(0.2, 0.25]     18\n",
      "(0.25, 0.3]     26\n",
      "(0.3, 0.35]     29\n",
      "(0.35, 0.4]     99\n",
      "(0.4, 0.45]     31\n",
      "(0.45, 0.5]     12\n",
      "(0.5, 0.55]      1\n",
      "(0.55, 0.6]      0\n",
      "(0.6, 0.65]      0\n",
      "(0.65, 0.7]      0\n",
      "(0.7, 0.75]      0\n",
      "(0.75, 0.8]      0\n",
      "(0.8, 0.85]      0\n",
      "(0.85, 0.9]      0\n",
      "(0.9, 0.95]      0\n",
      "(0.95, 1.0]      0\n",
      "Name: Count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file_path = 'hours-per-week_unique_values_counts.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Define bins for the intervals\n",
    "bins = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "# Create intervals and sum 'Count' values within each interval\n",
    "df['Interval'] = pd.cut(df['Value'], bins=bins, include_lowest=False)\n",
    "result_df = df.groupby('Interval')['Count'].sum()#.reset_index()\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(\"hours_bins.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
